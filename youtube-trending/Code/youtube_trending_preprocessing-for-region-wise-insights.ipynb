{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the final pre-processing code for the region insights in terms of foundational cleaning and merging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime as dt\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from api_keys import (gkey, gkey2, gkey3)\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the Data to a single DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_files(path, ty='csv', Name='Merged_DF.csv'):\n",
    "    #Iteratively appends all files with ty extention to list_of_files\n",
    "    for root,dirs,files in os.walk(path):\n",
    "        [list_of_files.append(file) for file in files if (file.endswith(f\".{ty}\") and (file!=Name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = 'Merged_DF.csv' #Name of Final DF\n",
    "list_of_files = []\n",
    "data_path = os.path.join('..', 'Data')\n",
    "find_all_files(data_path, Name=Name)\n",
    "\n",
    "Total_DF = pd.DataFrame()\n",
    "for file in list_of_files:\n",
    "    try:\n",
    "        DF = pd.read_csv(os.path.join('..', 'Data', file), encoding='utf-8')\n",
    "    except:\n",
    "        DF = pd.read_csv(os.path.join('..', 'Data', file), encoding='latin1')\n",
    "    DF['country'] = file[:2]\n",
    "    Total_DF = (DF if Total_DF.empty else pd.concat([Total_DF, DF]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total DF Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total_DF['video_id'].map(lambda x: type(x)!=str).sum() #All values are string\n",
    "#Total_DF['trending_date'].map(lambda x: type(x)!=str).sum() #All values are string\n",
    "#Convert the 'trending_date' to date format\n",
    "Total_DF['trending_date'] = Total_DF['trending_date'].map(lambda x: dt.strptime(x, \"%y.%d.%m\"))\n",
    "\n",
    "#Total_DF['title'].map(lambda x: type(x)!=str).sum() #All values are string\n",
    "#Total_DF['channel_title'].map(lambda x: type(x)!=str).sum() #All values are string\n",
    "#Total_DF['category_id'].map(lambda x: type(x)!=int).sum() #All values are int\n",
    "#Total_DF['publish_time'].map(lambda x: type(x)!=str).sum() #All values are string\n",
    "#Convert the 'publish_time' to date format\n",
    "Total_DF['publish_time'] = pd.to_datetime(Total_DF['publish_time'], format='%Y-%m-%dT%H:%M:%S.%fZ') #%f means microsecond which means 6 digits. This works here as it is always 0 microseconds\n",
    "#Total_DF['tags'].map(lambda x: type(x)!=str).sum() #All values are string\n",
    "#Total_DF['views'].map(lambda x: type(x)!=int).sum() #All values are int64\n",
    "#Total_DF['likes'].map(lambda x: type(x)!=int).sum() #All values are int64\n",
    "#Total_DF['dislikes'].map(lambda x: type(x)!=int).sum() #All values are int64\n",
    "#Total_DF['comment_count'].map(lambda x: type(x)!=int).sum() #All values are int64\n",
    "#Total_DF['likes'].isnull().sum()\n",
    "#Total_DF['dislikes'].isnull().sum()\n",
    "#Total_DF['comment_count'].isnull().sum()\n",
    "#Total_DF['thumbnail_link'].isnull().sum()\n",
    "#Total_DF['comments_disabled'].map(lambda x: type(x)!=bool).sum() #All values are boolean\n",
    "#Total_DF['ratings_disabled'].map(lambda x: type(x)!=bool).sum() #All values are boolean\n",
    "#Total_DF['video_error_or_removed'].map(lambda x: type(x)!=bool).sum() #All values are boolean\n",
    "#Total_DF['comments_disabled'].isnull().sum()\n",
    "#Total_DF['ratings_disabled'].isnull().sum()\n",
    "#Total_DF['video_error_or_removed'].isnull().sum()\n",
    "\n",
    "#Convert NaN values in 'description' to ''\n",
    "Total_DF['description'].fillna(value='', inplace=True)\n",
    "#Total_DF['description'].isna().sum()\n",
    "#Total_DF[Total_DF['description'] == ''].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing for duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360578, 17)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop the duplicate rows\n",
    "Total_DF.drop_duplicates(subset=['video_id', 'trending_date', 'country'], keep='last', inplace=True)#NEED TO CHANGE\n",
    "Total_DF.reset_index(drop=True, inplace=True)\n",
    "to_drop = Total_DF[(Total_DF['video_id']=='#NAME?') | (Total_DF['video_id']=='#VALUE!')].index\n",
    "Total_DF.drop(to_drop, inplace=True)\n",
    "Total_DF.reset_index(drop=True, inplace=True)\n",
    "Total_DF.shape\n",
    "#375942 - 14518 - 846  = 360578"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Videos with multiple publish Times and videos with 'video_error_or_removed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360432, 17)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(360217, 17)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(360217, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Publish time is supposed to be unique. Remove the videos with More than 1 publish time \n",
    "Temp_TF = Total_DF.groupby('video_id').aggregate(Publish_Time_Unique_Count = ('publish_time', lambda x : len(set(x.to_list()))))           \n",
    "#Drop these 29 videos, total 146 corresponding rows\n",
    "Total_DF.drop(Total_DF[Total_DF['video_id'].isin(Temp_TF[Temp_TF['Publish_Time_Unique_Count']>1].index)].index, inplace=True)\n",
    "Total_DF.reset_index(drop=True, inplace=True)\n",
    "Total_DF.shape\n",
    "#360578 - 146 = 360432\n",
    "\n",
    "#Some Videos are removed after some time; Let's exclude these videos from the analysis as there is a manual intervention or environment issue \n",
    "#50 videos had error (atleast once); Total 215 rows\n",
    "#Both the below codes give exactly same results; MEANS 'video_error_or_removed' really means that atleast once 'video_error_or_removed'\n",
    "#Total_DF[Total_DF['video_id'].isin(Total_DF[Total_DF['video_error_or_removed']]['video_id'].unique())]\n",
    "#Total_DF[Total_DF['video_error_or_removed']]\n",
    "Total_DF.drop(Total_DF[Total_DF['video_id'].isin(Total_DF[Total_DF['video_error_or_removed']]['video_id'].unique())].index, inplace=True)\n",
    "Total_DF.reset_index(drop=True, inplace=True)\n",
    "Total_DF.shape\n",
    "#360432-215 = 360217\n",
    "\n",
    "#Remove 'video_error_or_removed' as it doesn't carry any relevant info now\n",
    "Total_DF.drop('video_error_or_removed', inplace=True, axis=1)\n",
    "Total_DF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the DataFrame as pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total_DF.to_pickle(\"../Data/VideoDF.pkl\") #Commenting to avoid overwritting the existing file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video_DF = pd.read_pickle(\"../Data/VideoDF.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 360217 entries, 0 to 360216\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   video_id           360217 non-null  object        \n",
      " 1   trending_date      360217 non-null  datetime64[ns]\n",
      " 2   title              360217 non-null  object        \n",
      " 3   channel_title      360217 non-null  object        \n",
      " 4   category_id        360217 non-null  int64         \n",
      " 5   publish_time       360217 non-null  datetime64[ns]\n",
      " 6   tags               360217 non-null  object        \n",
      " 7   views              360217 non-null  int64         \n",
      " 8   likes              360217 non-null  int64         \n",
      " 9   dislikes           360217 non-null  int64         \n",
      " 10  comment_count      360217 non-null  int64         \n",
      " 11  thumbnail_link     360217 non-null  object        \n",
      " 12  comments_disabled  360217 non-null  bool          \n",
      " 13  ratings_disabled   360217 non-null  bool          \n",
      " 14  description        360217 non-null  object        \n",
      " 15  country            360217 non-null  object        \n",
      "dtypes: bool(2), datetime64[ns](2), int64(5), object(7)\n",
      "memory usage: 39.2+ MB\n"
     ]
    }
   ],
   "source": [
    "Video_DF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id                     object\n",
       "trending_date        datetime64[ns]\n",
       "title                        object\n",
       "channel_title                object\n",
       "category_id                   int64\n",
       "publish_time         datetime64[ns]\n",
       "tags                         object\n",
       "views                         int64\n",
       "likes                         int64\n",
       "dislikes                      int64\n",
       "comment_count                 int64\n",
       "thumbnail_link               object\n",
       "comments_disabled              bool\n",
       "ratings_disabled               bool\n",
       "description                  object\n",
       "country                      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking Sanity\n",
    "Video_DF.dtypes\n",
    "Video_DF.duplicated(subset=['video_id', 'trending_date', 'country'], keep='last').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Intruducing the Notion of Popularity. What makes trending videos popular ?\n",
    "\n",
    "**Popularity score (Longevity): Total days of trending for a video**\n",
    "\n",
    "**Populrity score of a trending video depends on what ?**\n",
    "\n",
    "**Useful metric to consider from the dataset**\n",
    "- Views of Trend Day 1\n",
    "- Likes of Trend Day 1\n",
    "- Dislikes of Trend Day 1\n",
    "- Comment_Count of Trend Day 1\n",
    "- Words in Title\n",
    "- Channel Title\n",
    "- Category ID\n",
    "- Tags\n",
    "- comments_disabled\n",
    "- ratings_disabled\n",
    "- video_error_or_removed\n",
    "- description\n",
    "\n",
    "**Extract the info outside dataset**\n",
    "- Publish time of the day based on the timezone of the channel (Need outside info)\n",
    "- Country\n",
    "- Language\n",
    "\n",
    "**Extract info outside news**\n",
    "- News effect on popularity (Can be speific to a category)\n",
    "- Google trending effect on popularity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Video Features via API call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample API call\n",
    "\n",
    "base = 'https://www.googleapis.com/youtube/v3/videos'\n",
    "\n",
    "\n",
    "params = {\n",
    "            'part' : 'snippet,contentDetails',\n",
    "            #'part' :'localizations',\n",
    "            #'part' : 'player',\n",
    "            #'part' : 'recordingDetails',\n",
    "            #'part' : 'statistics',\n",
    "            #'part' : 'status',\n",
    "            'id': <List of Videos>,\n",
    "            'key':gkey\n",
    "}\n",
    "\n",
    "response = requests.get(base, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "51\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#Video_ID_List = list(set(Video_DF['video_id'])) #COMMENTING NOT TO ACCEDENTALLY RUN THE ENTIRE BATCH\n",
    "#Video_ID_List = pickle.load(open( \"To_scrape.p\", \"rb\" )) #THIS IS USED TO SCRAPE MISSED VIDEOS\n",
    "\n",
    "Total = []\n",
    "len(Video_ID_List)\n",
    "Video_features = dict()\n",
    "base = 'https://www.googleapis.com/youtube/v3/videos'\n",
    "Total_Videos = len(Video_ID_List)\n",
    "count = 0\n",
    "for iteration in range(((len(Video_ID_List)//50 + 1) if len(Video_ID_List)%50 else len(Video_ID_List)//50)):\n",
    "    list_of_videos = (Video_ID_List[iteration*50:iteration*50+50] if (iteration+1)*50<=Total_Videos else Video_ID_List[iteration*50:])\n",
    "    Total = Total + list_of_videos\n",
    "    params = {\n",
    "            'part' : 'snippet,contentDetails',\n",
    "            'id': list_of_videos,\n",
    "            'key':gkey\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base, params)\n",
    "    \n",
    "    try:\n",
    "        content = response.json()\n",
    "        \n",
    "        for item in content['items']:\n",
    "            count+=1\n",
    "            ID = item['id']\n",
    "\n",
    "            Video_features[ID] = {'ChannelID' : (item['snippet']['channelId'] if 'channelId' in item['snippet'] else None)}\n",
    "\n",
    "            Video_features[ID].update({'ChannelTitle':(item['snippet']['channelTitle'] if 'channelTitle' in item['snippet'] else None)})\n",
    "\n",
    "            Video_features[ID].update({'DefaultLanguage' : (item['snippet']['defaultLanguage'] if 'defaultLanguage' in item['snippet'] else None)})\n",
    "\n",
    "            Video_features[ID].update({'DefaultAudioLanguage' : (item['snippet']['defaultAudioLanguage'] if 'defaultAudioLanguage' in item['snippet'] else None)})\n",
    "\n",
    "            Video_features[ID].update({'Duration' : (item['contentDetails']['duration'] if 'duration'in item['contentDetails'] else None)})\n",
    "\n",
    "            Video_features[ID].update({'Caption' : (item['contentDetails']['caption'] if 'caption'in item['contentDetails'] else None)})\n",
    "\n",
    "            Video_features[ID].update({'RegionRestriction_Blocked' : (((item['contentDetails']['regionRestriction']['blocked']  if  ('blocked' in item['contentDetails']['regionRestriction']) else None)) if ('regionRestriction' in item['contentDetails']) else None)})   \n",
    "\n",
    "            Video_features[ID].update({'RegionRestriction_Allowed' : (((item['contentDetails']['regionRestriction']['allowed']  if  ('allowed' in item['contentDetails']['regionRestriction']) else None)) if ('regionRestriction' in item['contentDetails']) else None)})   \n",
    "            \n",
    "    except:\n",
    "        print(response.url)\n",
    "        print(response)\n",
    "        print(f\"Didn't get response for iteration {iteration}\")\n",
    "        \n",
    "    time.sleep(2)\n",
    "    print(count)\n",
    "time_taken_in_min=(time.time()-start_time)//60 \n",
    "#Before I start,\n",
    "#YouTube Data API v3\t74\t\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124848, 9)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scraped_DF = pd.DataFrame(Video_features).transpose()\n",
    "#Remove duplicates if any\n",
    "#Scraped_DF=Scraped_DF[~Scraped_DF.duplicated(['video_id'], keep='first')]\n",
    "#Scraped_DF.to_pickle(\"../Data/API_RETRIEVED_DATA.pkl\")\n",
    "Scraped_DF = pd.read_pickle('../Data/API_RETRIEVED_DATA.pkl')\n",
    "Scraped_DF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Video Data with Scraped Data (left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360217, 16)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(124848, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Index(['video_id', 'trending_date', 'title', 'channel_title', 'category_id',\n",
       "       'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count',\n",
       "       'thumbnail_link', 'comments_disabled', 'ratings_disabled',\n",
       "       'description', 'country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Index(['video_id', 'ChannelID', 'ChannelTitle', 'DefaultLanguage',\n",
       "       'DefaultAudioLanguage', 'Duration', 'Caption',\n",
       "       'RegionRestriction_Blocked', 'RegionRestriction_Allowed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Index(['video_id', 'trending_date', 'title', 'channel_title', 'category_id',\n",
       "       'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count',\n",
       "       'thumbnail_link', 'comments_disabled', 'ratings_disabled',\n",
       "       'description', 'country', 'ChannelID', 'ChannelTitle',\n",
       "       'DefaultLanguage', 'DefaultAudioLanguage', 'Duration', 'Caption',\n",
       "       'RegionRestriction_Blocked', 'RegionRestriction_Allowed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(360217, 24)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video_DF.shape\n",
    "Scraped_DF.shape\n",
    "\n",
    "Video_DF.columns\n",
    "Scraped_DF.columns\n",
    "\n",
    "New_Video_DF = Video_DF.merge(Scraped_DF, how='left', on='video_id')\n",
    "New_Video_DF.columns\n",
    "New_Video_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New_Video_DF.to_pickle(\"../Data/New_Video_DF_360217.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
